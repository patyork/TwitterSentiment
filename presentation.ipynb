{
 "metadata": {
  "name": "",
  "signature": "sha256:c9c4358839e81affd313833880a517217e37ad90d09502ca2cb3c2210067b3db"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Giving Credit Where Credit is Due"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So it wouldn't be right if we didn't take a moment to give the guy who had the original idea a little recognition. Paul Hawtin is the co-founder of the hedge fund Cayman Atlantic. There are some interesting details about the first hedge fund he founded, DMC, and later auctioned off, but I\u2019ll leave that up to you all to investigate. If you\u2019re interested, here is Cayman Atlantic's website and some links to a few articles we saved, detailing the some of the companies' history. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML('<iframe src=http://www.caymanatlantic.com/?useformat=mobile width=1000 height=500></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe src=http://www.caymanatlantic.com/?useformat=mobile width=1000 height=500></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "<IPython.core.display.HTML at 0x3c17cd0>"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import FileLink, FileLinks\n",
      "FileLinks('dmc-caymanAtlantic')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "dmc-caymanAtlantic/<br>\n",
        "&nbsp;&nbsp;<a href='files/dmc-caymanAtlantic/.DS_Store' target='_blank'>.DS_Store</a><br>\n",
        "&nbsp;&nbsp;<a href='files/dmc-caymanAtlantic/dmc_auctioned.txt' target='_blank'>dmc_auctioned.txt</a><br>\n",
        "&nbsp;&nbsp;<a href='files/dmc-caymanAtlantic/dmc_closing.txt' target='_blank'>dmc_closing.txt</a><br>\n",
        "&nbsp;&nbsp;<a href='files/dmc-caymanAtlantic/dmc_opening.txt' target='_blank'>dmc_opening.txt</a><br>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "dmc-caymanAtlantic/\n",
        "  .DS_Store\n",
        "  dmc_auctioned.txt\n",
        "  dmc_closing.txt\n",
        "  dmc_opening.txt"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "First thing's Second"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What is 'sentiment'?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sentiment is a measure of the emotion within a body of text; positive or negative. Sentiment is a highly subjective quantity; in fact, humans will generally agree on the sentiment of some text about 80% of the time. We used the Natural Language Toolkit and its corresponding book as the main resource for text processing and sentiment analysis information."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Collection and Cleaning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Long story short, we used a supervised machine learning algorithm which meant that we needed some already classified data on which to train. The go-to data set for sentiment analysis is a corpus of Movie Reviews from Rotten Tomatoes or something similar; each review contains a rating (which can be distilled down to positive or negative) and a body of text. We expanded upon this by using a raw corpus of 36 million Amazon product reviews. The thought here was that a more diverse training set (as opposed to just movie reviews) would improve generalizability."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We pared the 36 million reviews down to the reviews which were either 1 star of 5 star out of a possible 5 stars; either really positive or really negative reviews. This list we pared down further to include only reviews over a certain length, and only reviews that had been rated as \u201cHelpful\u201d by the Amazon community. We ended up with a usable corpus of 1.4 million reviews."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also implemented an automatic spelling corrector, as spelling counts in the game of sentiment analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SpellingCorrector import SpellingCorrector as SC\n",
      "\n",
      "corrector = SC.SpellingCorrector()\n",
      "\n",
      "terribleSentence = \"This. is a saample reviiew/sntence thatshould show off.alll of it's abbillities :)\"\n",
      "\n",
      "for x in corrector.CorrectSpelling(terribleSentence):\n",
      "\tprint x,\n",
      "print ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "this sample review sentence that should show off all its abilities happy \n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analysis Phase"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Building an Estimator and Cross Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have acquired, reformatted and cleaned our data; we can turn our attention to building the estimator and cross validating our results.\n",
      "\n",
      "To build our training data we use Amazon's five star ratings and one star ratings, calling them positive and negative respectively. Then we put the reviews in an array and shuffle them. Usually we'd want to save a portion of the training data to create a test set, but because we use a method that incorporates cross validation, we don't need to do this.\n",
      "\n",
      "We are using a 'Pipeline' to perform two transformations and estimate our data. The first transformation is a 'CountVectorizer' that converts the Amazon reviews into matrices of token counts. The second transformation is a 'TfidTransformer' which scales down the impact of the frequently occurring tokens. For the estimator we use the 'SGDClassifier' which is a linear classifier with stochastic gradient descent learning.\n",
      "\n",
      "So...to help us decide what parameters will give us the best result with our 'Pipeline' and to cross validate our result, we use 'GridSearchCV.' This is a huge help in identifying what combination, of the many available parameter, gives us the best result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# python imports\n",
      "from packages.funcs import *\n",
      "import numpy as np\n",
      "from random import shuffle\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "from sklearn.linear_model import SGDClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# number of reviews to extract\n",
      "numElements = 100000\n",
      "\n",
      "# get the negative Amazom review data and create a list of tuples (review, neg-lable)\n",
      "x_neg = np.array(extractData('amazon-reviews/onestar-clean.txt.gz', numElements))\n",
      "y_neg = [0] * numElements\n",
      "train_neg = zip(x_neg, y_neg)\n",
      "\n",
      "# get the positive Amazom rewiew data and create a list of tuples (review, pos-lable)\n",
      "x_pos = np.array(extractData('amazon-reviews/fivestar-clean.txt.gz', numElements))\n",
      "y_pos = [1] * numElements\n",
      "train_pos = zip(x_pos, y_pos)\n",
      "\n",
      "# combine and shuffle the positive / negative data\n",
      "data = train_neg + train_pos\n",
      "shuffle(data)\n",
      "\n",
      "# take 1000 reviews to run GridSearch\n",
      "train = zip( * data[:1000])\n",
      "x_train, y_train = train[0], train[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create our pipeline with our transformers and classifier\n",
      "pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(n_jobs=-1))])\n",
      "\n",
      "# define our parameter that we want to test on our transformers and classifer during our grid search\n",
      "parameters = {'vect__max_df': (0.25, 0.5, 0.75, 1.0), # ignore terms that have a term frequency higher than threshold\n",
      "              'vect__max_features': (None, 5000, 10000, 50000), # builds a vocabulary with only the top max_features\n",
      "              'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)), # lower/upper boundary of range of n-values for different n-grams to be extracted\n",
      "              'tfidf__smooth_idf': (True, False), # smooth idf weights by adding one to document frequencies, prevents zero division\n",
      "              'tfidf__use_idf': (True, False), # inverse-document-frequency reweighting\n",
      "              'tfidf__norm': ('l1', 'l2'), # used to normalize term vectors\n",
      "              'clf__loss': ('hinge', #  linear support vector machine (SVM)\n",
      "                            'log', # logistic regression\n",
      "                            'modified_huber', # smooth loss that brings tolerance to outliers as well as probability estimates\n",
      "                            'squared_hinge', # is like hinge but is quadratically penalized\n",
      "                            'perceptron', # is the linear loss used by the perceptron algorithm\n",
      "                            'squared_loss', # ordinary least squares fit\n",
      "                            'huber', # modifies \u2018squared_loss\u2019 to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon\n",
      "                            'epsilon_insensitive', # ignores errors less than epsilon and is linear past that; this is the loss function used in SVR\n",
      "                            'squared_epsilon_insensitive' # same as epsilon_insensitive, but becomes squared loss past a tolerance of epsilon),\n",
      "              'clf__alpha': (0.00001, 0.000001, 0.0000001), # constant that multiplies the regularization term\n",
      "              'clf__penalty': ('l1', 'l2', 'elasticnet'), # penalty (aka regularization term) to be used\n",
      "              'clf__n_iter': (10, 20, 40, 80)} # number of passes over the training data\n",
      "\n",
      "# create our grid search object and fit it to our data\n",
      "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, pre_dispatch=4, cv=5, verbose=1)\n",
      "grid_search.fit(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 5 folds for each of 36864 candidates, totalling 184320 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.2s\n",
        "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   10.1s\n",
        "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   45.4s\n",
        "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.7min\n",
        "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.0min\n",
        "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  4.7min\n",
        "[Parallel(n_jobs=-1)]: Done 1800 jobs       | elapsed:  6.8min\n",
        "[Parallel(n_jobs=-1)]: Done 2450 jobs       | elapsed:  9.2min\n",
        "[Parallel(n_jobs=-1)]: Done 3200 jobs       | elapsed: 12.1min\n",
        "[Parallel(n_jobs=-1)]: Done 4050 jobs       | elapsed: 15.3min\n",
        "[Parallel(n_jobs=-1)]: Done 5000 jobs       | elapsed: 19.0min\n",
        "[Parallel(n_jobs=-1)]: Done 6050 jobs       | elapsed: 23.0min\n",
        "[Parallel(n_jobs=-1)]: Done 7200 jobs       | elapsed: 27.5min\n",
        "[Parallel(n_jobs=-1)]: Done 8450 jobs       | elapsed: 32.3min\n",
        "[Parallel(n_jobs=-1)]: Done 9800 jobs       | elapsed: 37.5min\n",
        "[Parallel(n_jobs=-1)]: Done 11250 jobs       | elapsed: 43.2min\n",
        "[Parallel(n_jobs=-1)]: Done 12800 jobs       | elapsed: 49.2min\n",
        "[Parallel(n_jobs=-1)]: Done 14450 jobs       | elapsed: 55.6min\n",
        "[Parallel(n_jobs=-1)]: Done 16200 jobs       | elapsed: 62.5min\n",
        "[Parallel(n_jobs=-1)]: Done 18050 jobs       | elapsed: 69.7min\n",
        "[Parallel(n_jobs=-1)]: Done 20000 jobs       | elapsed: 77.2min\n",
        "[Parallel(n_jobs=-1)]: Done 22050 jobs       | elapsed: 85.3min\n",
        "[Parallel(n_jobs=-1)]: Done 24200 jobs       | elapsed: 94.2min\n",
        "[Parallel(n_jobs=-1)]: Done 26450 jobs       | elapsed: 103.3min\n",
        "[Parallel(n_jobs=-1)]: Done 28800 jobs       | elapsed: 112.8min\n",
        "[Parallel(n_jobs=-1)]: Done 31250 jobs       | elapsed: 122.7min\n",
        "[Parallel(n_jobs=-1)]: Done 33800 jobs       | elapsed: 132.7min\n",
        "[Parallel(n_jobs=-1)]: Done 36450 jobs       | elapsed: 143.0min\n",
        "[Parallel(n_jobs=-1)]: Done 39200 jobs       | elapsed: 153.8min\n",
        "[Parallel(n_jobs=-1)]: Done 42050 jobs       | elapsed: 164.9min\n",
        "[Parallel(n_jobs=-1)]: Done 45000 jobs       | elapsed: 176.5min\n",
        "[Parallel(n_jobs=-1)]: Done 48050 jobs       | elapsed: 188.6min\n",
        "[Parallel(n_jobs=-1)]: Done 51200 jobs       | elapsed: 200.9min\n",
        "[Parallel(n_jobs=-1)]: Done 54450 jobs       | elapsed: 214.1min\n",
        "[Parallel(n_jobs=-1)]: Done 57800 jobs       | elapsed: 228.0min\n",
        "[Parallel(n_jobs=-1)]: Done 61250 jobs       | elapsed: 242.6min\n",
        "[Parallel(n_jobs=-1)]: Done 64800 jobs       | elapsed: 256.4min\n",
        "[Parallel(n_jobs=-1)]: Done 68450 jobs       | elapsed: 270.3min\n",
        "[Parallel(n_jobs=-1)]: Done 72200 jobs       | elapsed: 284.8min\n",
        "[Parallel(n_jobs=-1)]: Done 76050 jobs       | elapsed: 300.0min\n",
        "[Parallel(n_jobs=-1)]: Done 80000 jobs       | elapsed: 315.6min\n",
        "[Parallel(n_jobs=-1)]: Done 84050 jobs       | elapsed: 331.3min\n",
        "[Parallel(n_jobs=-1)]: Done 88200 jobs       | elapsed: 348.3min\n",
        "[Parallel(n_jobs=-1)]: Done 92450 jobs       | elapsed: 365.6min\n",
        "[Parallel(n_jobs=-1)]: Done 96800 jobs       | elapsed: 382.2min\n",
        "[Parallel(n_jobs=-1)]: Done 101250 jobs       | elapsed: 399.3min\n",
        "[Parallel(n_jobs=-1)]: Done 105800 jobs       | elapsed: 416.8min\n",
        "[Parallel(n_jobs=-1)]: Done 110450 jobs       | elapsed: 434.9min\n",
        "[Parallel(n_jobs=-1)]: Done 115200 jobs       | elapsed: 453.5min\n",
        "[Parallel(n_jobs=-1)]: Done 120050 jobs       | elapsed: 472.9min\n",
        "[Parallel(n_jobs=-1)]: Done 125000 jobs       | elapsed: 492.6min\n",
        "[Parallel(n_jobs=-1)]: Done 130050 jobs       | elapsed: 511.7min\n",
        "[Parallel(n_jobs=-1)]: Done 135200 jobs       | elapsed: 531.4min\n",
        "[Parallel(n_jobs=-1)]: Done 140450 jobs       | elapsed: 551.6min\n",
        "[Parallel(n_jobs=-1)]: Done 145800 jobs       | elapsed: 572.2min\n",
        "[Parallel(n_jobs=-1)]: Done 151250 jobs       | elapsed: 593.6min\n",
        "[Parallel(n_jobs=-1)]: Done 156800 jobs       | elapsed: 615.2min\n",
        "[Parallel(n_jobs=-1)]: Done 162450 jobs       | elapsed: 636.5min\n",
        "[Parallel(n_jobs=-1)]: Done 168200 jobs       | elapsed: 658.9min\n",
        "[Parallel(n_jobs=-1)]: Done 174050 jobs       | elapsed: 681.6min\n",
        "[Parallel(n_jobs=-1)]: Done 180000 jobs       | elapsed: 705.2min\n",
        "[Parallel(n_jobs=-1)]: Done 184318 out of 184320 | elapsed: 722.3min remaining:    0.5s\n",
        "[Parallel(n_jobs=-1)]: Done 184320 out of 184320 | elapsed: 722.3min finished\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "GridSearchCV(cv=5,\n",
        "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), prep...ower_t=0.5,\n",
        "       random_state=None, rho=None, shuffle=False, verbose=0,\n",
        "       warm_start=False))]),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=-1,\n",
        "       param_grid={'vect__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4)), 'tfidf__smooth_idf': (True, False), 'clf__loss': ('hinge', 'log'), 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'vect__max_df': (0.25, 0.5, 0.75, 1.0), 'clf__penalty': ('l1', 'l2', 'elasticnet'), 'clf__n_iter': (10, 20, 40, 80), 'vect__max_features': (None, 5000, 10000, 50000), 'clf__alpha': (1e-05, 1e-06, 1e-07)},\n",
        "       pre_dispatch=4, refit=True, score_func=None, scoring=None,\n",
        "       verbose=1)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# display our results\n",
      "print 'Best Score:\\n\\t~ %0.3f' % grid_search.best_score_\n",
      "best_parameters = grid_search.best_estimator_.get_params()\n",
      "print 'Best Parameters:'\n",
      "for param_name in sorted(parameters.keys()):\n",
      "    print '\\t~', param_name, best_parameters[param_name]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best Score:\n",
        "\t~ 0.851\n",
        "Best Parameters:\n",
        "\t~ clf__alpha 1e-07\n",
        "\t~ clf__loss hinge\n",
        "\t~ clf__n_iter 80\n",
        "\t~ clf__penalty l1\n",
        "\t~ tfidf__norm l1\n",
        "\t~ tfidf__smooth_idf True\n",
        "\t~ tfidf__use_idf True\n",
        "\t~ vect__max_df 0.25\n",
        "\t~ vect__max_features 50000\n",
        "\t~ vect__ngram_range (1, 2)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classification and Perdiction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the best parameters for our 'Pipeline' lets test it out on some tweets. These tweets have been run through our spelling corrector and have been manually assigned negative and positive values. Notice the difference in the scores of our training data versus our testing data. We also make a quick check to make sure its not just a lucky predictions. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# python imports\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# take all 100k reviews to train classifier\n",
      "train = zip( * data)\n",
      "x_train, y_train = train[0], train[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get the pre rated tweets\n",
      "with open('twitter-tweets/ratedTweetsCorrected.txt', 'r') as f:\n",
      "    ratedTweets = f.readlines()\n",
      "    \n",
      "# clean the tweets and put into arrays\n",
      "x_tweets = []\n",
      "y_tweets = []\n",
      "for ratedTweet in ratedTweets:\n",
      "    y, x = ratedTweet.strip().split('|;|')\n",
      "    x_tweets.append(cleanTweet(x).strip()), y_tweets.append(int(y))\n",
      "    \n",
      "# convert to numpy arrays\n",
      "x_tweets = np.array(x_tweets)\n",
      "y_tweets = np.array(y_tweets)\n",
      "\n",
      "# convert all -1 to 0\n",
      "y_tweets[y_tweets == -1] = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create our pipeline with our transformers and classifier\n",
      "pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(n_jobs=-1))])\n",
      "\n",
      "# set parameters bases on GridSearch results\n",
      "pipeline.set_params(vect__max_df=0.5, vect__max_features=50000, vect__ngram_range=(1, 3),\n",
      "                    tfidf__norm='l2', tfidf__smooth_idf=True, tfidf__use_idf=True, \n",
      "                    clf__alpha=1e-05, clf__loss='hinge', clf__n_iter=80, clf__penalty='l2')\n",
      "\n",
      "# fit the training data\n",
      "pipeline.fit(x_train, y_train)\n",
      "\n",
      "# get predictions\n",
      "predictions = pipeline.predict(x_tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get score\n",
      "print 'prediction score: %0.3f%c' % ((pipeline.score(x_tweets, y_tweets) * 100), '%')\n",
      "\n",
      "# validate that clasifier is not predicting all as neg or pos\n",
      "tuples = zip(y_tweets, predictions)\n",
      "neg = pos = neg_wrong = pos_wrong = 0\n",
      "for tuple_ in tuples:\n",
      "    if tuple_[0] == 0: neg += 1\n",
      "    if tuple_[0] == 0 and tuple_[1] == 1: neg_wrong += 1\n",
      "    if tuple_[0] == 1: pos += 1\n",
      "    if tuple_[0] == 1 and tuple_[1] == 0: pos_wrong += 1\n",
      "print 'percentage of incorrect predictions that are negative: %0.3f%c and positive: %0.3f%c' % (((neg_wrong / neg) * 100), '%', ((pos_wrong / pos) * 100), '%')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "prediction score: 73.333%\n",
        "percentage of incorrect predictions that are negative: 30.739% and positive: 22.761%\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://patyork.me:8080\">Live Twitter Sentiment</a>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So this\u2026.is a live graph of the sentiment of Twitter. Top graph is 5 second intervals and the bottom is 15 minute intervals. We get between 10 and 20 tweets per second, so this entire system has to clean and correct about 15 tweets, sentimentize them using the classifier, and dump the results off to a MySQL database each second, where this graph then uses the public interface via PHP and JSON to display the updated statistics in 5 and 900 second intervals. In other words, we just processed about 600 tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Reflection"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Make Comparisons and Explore Alternatives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some of the biggest problems, in building the classifier, come from machine performance. The first time we ran our \u2018GridSearch,\u2019 it had been processing for a little over two days and we received a floating point error; though, the error message was not clear as to what was causing the floating point error or where it was originating from. It kind of sucks to try and guess where the problem is stemming from, to just run another two days and get the same error. \n",
      "\n",
      "When trying to find the best number of Amazon reviews to train on, we ran into ram issues. Yet again, we would be running for several hours before we would receive an error message, just to adjust our input and try again. \n",
      "\n",
      "We never got around to trying it, but had started looking at Amazon\u2019s High Performance Computing (HPC). In addition, Continuum Analytics has IOPro that reduces the amount of memory necessary to load data structure from memory and Accelerate which opens up the capabilities of your GPU and multi-core processors to Python."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML('<iframe src=http://xkcd.com/303/?useformat=mobile width=825 height=500></iframe>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<iframe src=http://xkcd.com/303/?useformat=mobile width=825 height=500></iframe>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "<IPython.core.display.HTML at 0x3c17a90>"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Expanding to the Stock Market"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We would have liked to have tried to find a correlation between this sentiment that we are calculating and the prices of various stocks. This was our original goal, but it was apparently out of the scope of a 2 and a half month project, given that stock data is not free. We do plan to continue with this over the summer, and of course, all of our code is on GitHub, so we will race you to riches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}